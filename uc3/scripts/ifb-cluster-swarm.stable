#! /bin/bash

#### Script Create Swarm Cluster 
# implement creation from :
#   - cluster nodes (/root/cluster/cluster.disco)
#   - token (/root/cluster/.swarm-token) 
#
# Other file:
# - /root/cluster/cluster_init : when a cluster script has been runned one time
# - on all nodes in cluster (/tmp/config_node_swarm)
#
####

BASH=~/.bashrc
NODE=""
consul_version=0.6.4
D=`date +%Y%m%d_%H%M%S`

# Shared volume in cluster
WORK_DIR=/root/mydisk
INCLUDE_CONFIG=yes
CLUSTER_DIR=/root/cluster
log=${CLUSTER_DIR}/shipyard.log

#ad list useful scripts, separared by space
DIR=/ifb/bin/
#DIR=/root/swarm_cyclone/
CONFIG_SCRIPT="preconfigure_nodes.sh"
CONFIG_PATH=${DIR}${CONFIG_SCRIPT}
usefull_scripts="${CONFIG_PATH}" 

# Variable to create shared volume
USE_NFS_SERVER=no
PACKAGES="nfs-kernel-server"
EXPORTS_F=/etc/exports
EXPORTS_RULES="(rw,no_root_squash,subtree_check)"

ldocker='docker -H tcp://0.0.0.0:2375 --cluster-advertise eth0:2375'
swarmdocker='docker -H tcp://0.0.0.0:5732 --cluster-advertise eth0:2375'

nodes_swarm=/root/cluster/cluster.disco
token=/root/cluster/.swarm-token
mkdir /root/cluster &> /dev/null

IP_MASTER=$(ifconfig eth0 | grep 'inet ' | tr -s ' ' | awk -F '[ :]' '{print $4}' )


########################################################################################################################
#
### function
#
########################################################################################################################
function msglog() 
{
   	echo -e "$@" | tee -a $log
}

function msgerr() 
{ 
	msglog "================== \n ERROR $@"
}


function get_ipnodes()
{
	less $NODE | grep -v "#"
}


function run_command2vm()
{
	ip_node=$1
	cmd=$2

	if [ "X$1" == "X" ]; then msgerr "Run command2vm: missing ip node." ; exit 1 ; fi
	if [ "X$2" == "X" ]; then msgerr "Run command2vm: missing command." ; exit 1 ; fi
	
	msglog "Command ssh  ssh $SSH_OPTIONS root@${ip_node} $cmd"
	ssh -o StrictHostKeyChecking=no -p 22 root@${ip_node} $cmd

	if [ $? -ne 0 ]; then
		msgerr "command: $cmd , failed on VM ${ip_node}"
		# exit 1
	else
		msglog "SUCCESS execute command $cmd  on VM ${ip_node}"
	fi
}

function check_nodes_exists()
{
	# check 
	status=0

	for ip in $(get_ipnodes)
	do
		msglog "slave with ip $ip"
		
		run_command2vm $ip "ls" &> /dev/null

		if [ $? -eq 0 ]
		then
			msglog "Test connexion $ip successed"
		else
			msgerr "ERROR test connexion on $ip. Check machine exist or IP valid "
			status=1
		fi
		
		if [ $status -ne 0 ]
		then
			msgerr "FAIL create shared volume, one or more slaves not found status $status"
			exit 1
		fi
	done
}

function restart_docker()
{
    service docker stop 
	service docker start 

	# Check succesful running service
	docker info

	if [ $? -ne 0 ]
	then
		msgerr "FAIL to restart service Docker"
		exit 1
	else
		msglog "Successfull restart service Docker with pid : $(less /var/run/docker.pid)"
	fi
}


########################################################################################################################
# Master
########################################################################################################################

function config_master()
{
	msglog "Configure master $IP_MASTER"
	
	# Run script
	${CONFIG_PATH}
}




function consul_master ()
{
	msglog "Run consul agent on master $IP_MASTER"
	msglog "DEBUG consul agent -server -bootstrap-expect 1 -data-dir /tmp/consul -node=master -bind=$IP_MASTER -config-dir /etc/consul.d &> ${CLUSTER_DIR}/consul_master_${IP_MASTER}_${D}.log "
	consul agent -server -bootstrap-expect 1 -data-dir /tmp/consul -node=master -bind=$IP_MASTER -config-dir /etc/consul.d &> ${CLUSTER_DIR}/consul_master_${IP_MASTER}_${D}.log & 
	sleep 2
	if [ $(consul members | grep master | wc -l) -ne 1 ]
	then
		msgerr "consul agent failed add master in members list"
		exit 1
	else
		msglog "SUCCESS add master in consul agent"
	fi
}

########################################################################################################################
# Shared volume
########################################################################################################################

function create_shared_volume()
{

	msglog "Create shared volume on master"
	mkdir -p $WORK_DIR &> /dev/null
	umount -f $WORK_DIR &> /dev/null

	if [ "$USE_NFS_SERVER" == "yes" ]; then 

		msglog "Add access to mydisk from NFS Server to master"
		run_command2vm $IP_DATA "echo -e '${WORK_DIR}\t${IP_MASTER}${EXPORTS_RULES}' >> $EXPORTS_F"
		run_command2vm $IP_DATA "service nfs reload"
		m=$(ssh root@${IP_DATA} grep -c $IP_MASTER $EXPORTS_F)		
		if [ $m -eq 0 ];then msgerr "FAIL to add $IP_MASTER in $EXPORTS_F file in NFS server $IP_DATA." ; exit 1; fi

		msglog "Mount mydisk from NFS server on Master"
		mount ${IP_DATA}:/$WORK_DIR $WORK_DIR
	else 

		install_nfs_server

		msglog "Create work dir on master "
		mkdir -p $WORK_DIR

		# Add configuration to slaves
		# create tmp file
		tmp_f=/tmp/exports.tmp
		if [ -f $EXPORTS_F ]; then
			mv $EXPORTS_F $tmp_f
		fi
	fi
}

function install_nfs_server()
{
	msglog "### NFS server : configure and run"
	apt-get update &> /dev/null

	if [ $? -ne 0 ]; then msglog "FAIL update machine" ; exit 1 ; fi

	msglog "NFS install package."
	apt-get install -y $PACKAGES &> /dev/null

	if [ $? -ne 0 ]; then msgerr "install $PACKAGES on system"; else msglog "SUCCESS $PACKAGES installed."; fi

	# Check install
	# service nfs-kernel-server status
	msglog "end install configure NFS"
}

########################################################################################################################
# Nodes
########################################################################################################################

function add_nodes()
{
	local tmp=/tmp/newnodes
	local count=0
	rm $tmp &> /dev/null

	for l in $(get_ipnodes)
	do 
		count=$(( count + 1 ))	
		# Check new node
		n=$($swarmdocker info 2> /dev/null | grep -c ${l}:2375 )
		
		if [ $n -eq 1 ]; then
			msglog "Node $l already included in cluster."
		else
			msglog "Add new node in cluster $l, number $count"
			echo ${l}:${count} >> $tmp
		fi
	done


	# Add new file		
	if [ "$INCLUDE_CONFIG" == "yes" ]; then
			# Configure node
			config_node $tmp
	fi 
		
	# Install consul
	consul_node $tmp
	
	# Create exports file with node IP
	build_exports_file $tmp

	# reload NFS server
	reload_nfs_server

	# Mount volume
	mount_node $tmp

	# Clean remove temporary file
	rm $tmp
}


function config_node()
{
	
	local newnodes=$1

	for val in $(less $newnodes ); do
		ip_node=$(echo $val | cut -d':' -f1)
		count=$(echo $val | cut -d':' -f2)

		msglog "Stop running Shipyard service."		
	    run_command2vm $ip_node "service shipyard stop &> /dev/null"
		run_command2vm $ip_node "service shipyard clean &> /dev/null"

		msglog "Configure node $ip_node"

		# Copy script on node
		scp -P 22 $CONFIG_PATH root@${ip_node}:/tmp
		
		if [ $? -ne 0 ];then
			msgerr "copy script $CONFIG_SCRIPT in VM  $ip_node"
			exit 1
		fi
		# Run script
		run_command2vm $ip_node "/tmp/$CONFIG_SCRIPT"
		
		# Remove script
		run_command2vm $ip_node "rm -f /tmp/$CONFIG_SCRIPT"
	done
}


function consul_node() 
{
	local newnodes=$1

	for val in $(less $newnodes ); do
		ip_node=$(echo $val | cut -d':' -f1)
		count=$(echo $val | cut -d':' -f2)

		hn=slave_$count
		msglog "Run consul on node $ip_node, hostname $hn"

		# run_command2vm $ip_node $cmd
		run_command2vm $ip_node "consul agent -data-dir /tmp/consul -node=agent_${hn} -bind=$ip_node -config-dir /etc/consul.d &> consul_node_${ip_node}_${D}.log &"

        # DEBUG
        msglog "DEBUG: sleep 2 to wait complete running consul on node"
        sleep 2

		msglog "Add node with: consul join $ip_node"
		consul join $ip_node

		if [ $? -ne 0 ]
		then
			msgerr "$n not possible to join on cluster with consul"
			exit 1
		else
			msglog "SUCCESS join $ip_node to cluster with consul"
		fi
	done
}

function build_exports_file()
{
	# Create exports file with node IP
	if [ "$USE_NFS_SERVER" == "yes" ]; then 
		build_exports_file_on_nfsserver $@
	else 
		build_exports_file_on_master $@
	fi
}

function build_exports_file_on_nfsserver()
{
	local newnodes=$1

	for val in $(less $newnodes ); do
		ip_node=$(echo $val | cut -d':' -f1)
		
		n=$(ssh root@${IP_DATA} grep -c $ip_node $EXPORTS_F )
		if [ $n -ge 1 ]; then
			# Node already setting
			msglog "Node already include in $EXPORTS_F file."
			continue
		fi

		run_command2vm $IP_DATA "echo -e '${WORK_DIR}\t${ip_node}${EXPORTS_RULES}' >> $EXPORTS_F"
		m=$(ssh root@${IP_DATA} grep -c $ip_node $EXPORTS_F)
		
		if [ $m -eq 0 ];then msgerr "FAIL to add $ip_node in $EXPORTS_F file in NFS server $IP_DATA." ; exit 1; fi
	done
}

function build_exports_file_on_master()
{
	local newnodes=$1

	for val in $(less $newnodes ); do
		ip_node=$(echo $val | cut -d':' -f1)
	
		n=$(grep -c $ip_node $EXPORTS_F)
		if [ $n -ge 1 ]; then
			# Node already setting
			msglog "Node already include in $EXPORTS_F file."
			continue
		fi

		echo -e "${WORK_DIR}\t${ip_node}${EXPORTS_RULES}" >> $EXPORTS_F
		if [ $? -ne 0 ];then msgerr "FAIL to add $ip_node in $EXPORTS_F" ; exit 1; fi
	done
}

function reload_nfs_server()
{
	# reload NFS server
	if [ "$USE_NFS_SERVER" == "yes" ]; then 
		reload_nfs_server_on_nfsserver
	else
		reload_nfs_server_master		
	fi
}

function reload_nfs_server_on_nfsserver()
{
	msglog "Restart NFS server $IP_DATA"
	
	# Restart NFS server
	run_command2vm $IP_DATA "service nfs reload"

    msglog "Check dir export"
	run_command2vm $IP_DATA "exportfs -av"
}

function reload_nfs_server_master()
{
	msglog "Restart nfs kernel server"
	# Restart NFS server
	service nfs-kernel-server stop
	service nfs-kernel-server start

    msglog "List machine in exports file \n $(less $EXPORTS_F)"

	msglog "Check dir export"
	exportfs -av
	
	msglog "Check directory exported:"

	if [ $(exportfs -av | wc -l) -eq 0 ]
	then
		msgerr "export $WORK_DIR failed"
		exit 1
	else
		msglog "SUCCESS export $WORK_DIR"
	fi
}


function mount_node()
{
	local newnodes=$1
	
	for val in $(less $newnodes ); do
		ip_node=$(echo $val | cut -d':' -f1)
		
		msglog "Mount $WORK_DIR on $ip_node"

		msglog "DEBUG ssh root@${ip_node} mkdir -p $WORK_DIR &> /dev/null"
		ssh -o StrictHostKeyChecking=no root@${ip_node} mkdir -p $WORK_DIR &> /dev/null

		msglog "DEBUG ssh root@${ip_node} umount -f $WORK_DIR"
		ssh -o StrictHostKeyChecking=no root@${ip_node} umount -f $WORK_DIR &> /dev/null

		msglog "DEBUG ssh root@${ip_node} mount ${IP_DATA}:/$WORK_DIR $WORK_DIR"
		ssh -o StrictHostKeyChecking=no root@${ip_node} mount ${IP_DATA}:$WORK_DIR $WORK_DIR
		if [ $? -ne 0 ]; then msgerr "Fail to mount workdir on node $ip_node."; exit 1 ; fi
	done
}

########################################################################################################################
# Run Swarm
########################################################################################################################


function run_swarm_modefile()
{
	# Check Swarm already running
	$swarmdocker ps &> /dev/null

	if [ $? -eq 0 ]; then
		msgerr "Swarm cluster alreay running, You should stop cluster to reconfigure them."
		exit 0
	fi

	msglog "Create file with <ip_node>:<port> to run swarm"
	rm $nodes_swarm &> /dev/null

	for l in $(get_ipnodes)
	do 
		echo ${l}:2375 >> $nodes_swarm
	done	

 	if [ $(get_ipnodes | wc -l) -ne $(less $nodes_swarm | wc -l) ]
 	then
 		msgerr "fail to create nodes file to run swarm cluster $nodes_swarm"
 		exit 1
 	fi

	msglog "Run swarm master"
	echo "DEBUG $ldocker run --name swarm -v $(dirname $nodes_swarm):/tmp -d -p 5732:2375 swarm manage file:///tmp/$(basename $nodes_swarm)"
	$ldocker run --name swarm -v $(dirname $nodes_swarm):/tmp -d -p 5732:2375 swarm manage file:///tmp/$(basename $nodes_swarm)
}

function run_swarm_modetoken()
{

	$swarmdocker ps &> /dev/null

	if [ $? -ne 0 ]; then
		$ldocker run --rm swarm create > $token
		msglog "Create cluster Swarm with token $(cat $token) "

		msglog "Run swarm manager"
		$ldocker run -d --name=master -p 5732:2375 swarm manage token://$(cat $token)
	else
		msglog "Swarm cluster already running."
	fi
	
	msglog "Join node in Swarm cluster"
	for l in $(get_ipnodes)
	do
		n=$( $ldocker ps | grep node-$l | wc -l )

		if [ $n -eq "1" ];then
			msglog "Swarm cluster : $l already joined."
		else
			$ldocker run -d --name=node-$l swarm join --addr=${l}:2375 token://$(cat $token)
			msglog "Swarm cluster : $l success joined at cluster"
		fi
	done
}

########################################################################################################################
# uninstall cluster
########################################################################################################################

function uninstall_cluster()
{
	# Confirm demand to uninstall cluster
	echo "Confirm demand to uninstall cluster : "
	read rep 

	case $rep in
        [Yy]* )

		for l in $(get_ipnodes)
		do 
			msglog "Clean node $l"
			run_command2vm $l /ifb/bin/uninstall_swarm.sh &> /tmp/uninstall_cluster.log
		done

		msglog "Clean master"
		/ifb/bin/uninstall_swarm.sh &> /tmp/uninstall_cluster.log
         ;;  


        * )  ;;
	esac

}

########################################################################################################################
#
### Main
#
########################################################################################################################
if [ -h $0 ]; then realfile=$(readlink $0) ; else readfile=$0 ; fi

usage="$(basename "$realfile") <path_to_node_file> [nfs-server-ip] -- program to create Cluster Swarm 
one argument : path to the node.l file which contains all IP nodes.
Optional : if data directory is mounted from NFS server, set the IP on server.
Use option 'uninstall' to uninstall cluster."

### Main

# Remove old log file
# rm $log

# Check arguments
if [ $# -eq 0 ]
then
	msgerr "Missing arguments in command line\n\tset path to node.l file included ip node."
	echo -e "\n${usage}\n"
	exit 1
fi

NODE=$1

if [ ! -f $NODE -o -z $NODE ]
then
	msgerr "Node file $NODE not exists or is empty."
	echo -e "\n${usage}\n"
	exit 1
fi


if [ $# -eq 1 ];then
	IP_DATA=$IP_MASTER
	msglog "Data directory is mount on master node. $IP_DATA"
else
	if [ $2 == "uninstall" ]
	then
		msglog "Ask to uninstall existing cluster"
		uninstall_cluster
		exit 0
	else
		IP_DATA=$2
		USE_NFS_SERVER=yes
		msglog "Set the IP on NFS server $IP_DATA"

		ssh -o StrictHostKeyChecking=no -p 22 root@$IP_DATA ls &> /dev/null
		if [ $? -ne 0 ];then msgerr "NFS server not accessible $IP_DATA"; exit 1; fi
	fi
fi

# Check all nodes exists
check_nodes_exists


if [ ! -f /root/cluster/cluster_init ]; then

	msglog "Stop Shipyard service"
	if [ $( service shipyard status | grep "is running"  &> /dev/null ; echo $? ) -eq 0 ];then 
		service shipyard stop
		service shipyard clean
	fi


	# Configure all nodes
	if [ "$INCLUDE_CONFIG" == "yes" ]; then
		config_master
	fi 

	# Create cluster
	consul_master

	# Create shared volume
	create_shared_volume
	touch /root/cluster/cluster_init
fi

# Add nodes in cluster
add_nodes

# Run swarm
# run_swarm_modefile
run_swarm_modetoken

# Keep time to load cluster nodes
sleep 10
$swarmdocker info
msglog "\nSource .bashrc file."

